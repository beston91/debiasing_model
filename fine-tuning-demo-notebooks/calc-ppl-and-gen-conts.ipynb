{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part II: Calculate Perplexity and Generate Continuations",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PART II: Calculating Perplexity and Generating Continuations\n",
        "Created March 20, 2022 | Last Updated: March 20, 2022\n",
        "\n",
        "This is PART II notebook (PART I is fine tuning) for calculating model perplexity and generating the continuations for that model. The output of this notebook is model perplexity (PPL) and continuations of the challenging dataset pushed to Huggingface. "
      ],
      "metadata": {
        "id": "cnhoQCv5dSRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before running the notebook: \n",
        "- Ensure you have changed your runtime to use the GPU and have High-RAM enabled to load GPT2-XL\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g9Bfz9MkdUsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOCUl48TdMHB"
      },
      "outputs": [],
      "source": [
        "token = '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'\n",
        "! git clone https://$token@github.com/beston91/debiasing_model.git\n",
        "\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install huggingface_hub\n",
        "!pip install nlp\n",
        "!apt install git-lfs\n",
        "\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast, pipeline, Trainer, TrainingArguments\n",
        "from huggingface_hub import notebook_login, Repository\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset as hf_load_dataset\n",
        "from nlp import load_dataset\n",
        "from tqdm import tqdm\n",
        "from typing import List, Optional, Tuple\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Add your huggingface from your account\n",
        "token = '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' # Beston's key \n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup paths\n",
        "\n",
        "Enter huggingface repo name here"
      ],
      "metadata": {
        "id": "BDH7hpfpcbTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter the huggingface repo below\n",
        "huggingface_repo = \"beston91/gpt2-xl_ft_logits_25k\"\n",
        "# huggingface_repo = \"IsaacSST/gpt2-xl-ft-d4-0.15-n-3\"\n",
        "user, MODEL_ID = huggingface_repo.split(\"/\")\n",
        "model_path = f\"{MODEL_ID}\" \n",
        "repo = Repository(local_dir=model_path, clone_from=f\"{user}/{model_path}\")\n",
        "repo.git_pull()"
      ],
      "metadata": {
        "id": "aYCNgX5SdbUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the perplexity"
      ],
      "metadata": {
        "id": "WB1fA1IbdeEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import modeling file\n",
        "sys.path.insert(1, '/content/debiasing_model/self-debiasing-timo')\n",
        "from modeling import ModelWrapper"
      ],
      "metadata": {
        "id": "8Qv-rTmkdfbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Wrapper(ModelWrapper):\n",
        "\n",
        "    def __init__(self, model_name: str = \"newtonkwan/gpt2-xl-ft-with-non-challenging-10k\", tokenizer: any = GPT2Tokenizer.from_pretrained(\"gpt2-xl\"), use_cuda: bool = True):\n",
        "        \"\"\"\n",
        "        :param model_name: the name of the pretrained GPT2 model (default: \"gpt2-xl\")\n",
        "        :param use_cuda: whether to use CUDA\n",
        "        \"\"\"\n",
        "        super().__init__(use_cuda=use_cuda)\n",
        "        self._tokenizer = tokenizer\n",
        "        self._model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        if use_cuda:\n",
        "            self._model.parallelize()\n",
        "        self._tokenizer.pad_token = self._tokenizer.eos_token\n",
        "        self._model.config.pad_token_id = self._tokenizer.eos_token_id\n",
        "\n",
        "    def query_model_batch(self, input_texts: List[str]):\n",
        "        inputs = self._tokenizer.batch_encode_plus(input_texts, padding=True, return_tensors='pt')\n",
        "        inputs = {key: val.to(self._device) for key, val in inputs.items()}\n",
        "        output_indices = inputs['attention_mask'].sum(dim=1) - 1\n",
        "        output = self._model(**inputs)['logits']\n",
        "        return torch.stack([output[example_idx, last_word_idx, :] for example_idx, last_word_idx in enumerate(output_indices)])\n",
        "\n",
        "    def generate(self, input_text: str, **kwargs):\n",
        "        input_ids = self._tokenizer.encode(input_text, return_tensors='pt').to(self._device)\n",
        "        generated_output = self._model.generate(input_ids, **kwargs)\n",
        "        output_ids = generated_output[0]\n",
        "        return self._tokenizer.decode(output_ids)\n",
        "\n",
        "    def compute_loss(self, input_ids: torch.LongTensor, labels: torch.LongTensor) -> torch.Tensor:\n",
        "        outputs = self._model(input_ids, labels=labels)\n",
        "        lm_logits = outputs[1]\n",
        "\n",
        "        # Shift so that tokens < n predict n\n",
        "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        # Flatten the tokens\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        return loss\n",
        "    def generate_self_debiasing():\n",
        "      ...\n",
        "    def compute_loss_self_debiasing():\n",
        "      ...\n"
      ],
      "metadata": {
        "id": "pZrC8QYMdwmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(output_filename,model='gpt2-xl',epsilon=0.01,max_length=-1,max_length_pattern=32,stride=-1,no_cuda=False,debug=False):\n",
        "  tokenizer = GPT2TokenizerFast.from_pretrained(model)\n",
        "  wrapper = GPT2Wrapper(model, use_cuda=not no_cuda)\n",
        "  device = 'cuda' if not no_cuda else 'cpu'\n",
        "\n",
        "  test = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "  encodings = tokenizer('\\n\\n'.join(test['text']), return_tensors='pt')\n",
        "\n",
        "  max_length = (max_length if max_length > 0 else wrapper._model.config.n_positions) - max_length_pattern\n",
        "\n",
        "  if stride <= 0:\n",
        "    stride = max_length\n",
        "\n",
        "  lls= []\n",
        "  ppl = None\n",
        "\n",
        "  for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
        "      begin_loc = max(i + stride - max_length, 0)\n",
        "      end_loc = min(i + stride, encodings.input_ids.size(1))\n",
        "      trg_len = end_loc - i  # may be different from stride on last loop\n",
        "      input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "      target_ids = input_ids.clone()\n",
        "      target_ids[:, :-trg_len] = -100\n",
        "\n",
        "      with torch.no_grad():\n",
        "            loss = wrapper.compute_loss(input_ids, labels=target_ids)\n",
        "            \n",
        "            log_likelihood = loss * trg_len\n",
        "\n",
        "      lls.append(log_likelihood)\n",
        "\n",
        "      ppl = torch.exp(torch.stack(lls).sum() / end_loc)\n",
        "      \n",
        "      print(f'Perplexity  after {i} tokens: {ppl}')\n",
        "\n",
        "  print(f'Final perplexity: {ppl}')\n",
        "\n",
        "  with open(output_filename, 'a', encoding='utf8') as fh:\n",
        "      fh.write(f'=== RESULT [{model}] ===\\n')\n",
        "      fh.write(f'Perplexity:  {ppl}\\n\\n')\n",
        "\n",
        "      return ppl"
      ],
      "metadata": {
        "id": "NfbovzPUdzVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity \n",
        "ppl = perplexity(model = model_path, output_filename = f\"/content/{MODEL_ID}/perplexity.txt\",max_length=992)"
      ],
      "metadata": {
        "id": "u4stddHcd03-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the perplexity score and dataset size to the README.md \n",
        "readme_file = f\"{MODEL_ID}/README.md\"\n",
        "with open(readme_file, 'a') as fp:\n",
        "  fp.write('\\n')\n",
        "  fp.write('\\n')\n",
        "  fp.write('### Perplexity')\n",
        "  fp.write('\\n')\n",
        "  fp.write(f\"Score: {str(ppl.item())}\")\n",
        "\n",
        "repo.push_to_hub(commit_message=\"Add perplexity score\")"
      ],
      "metadata": {
        "id": "nIxGhKmENfj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate continuations"
      ],
      "metadata": {
        "id": "5nKxm_J5Nm5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate continuations and push to huggingface hub\n",
        "PATH = \"/content/debiasing_model/model-input/prompts\"\n",
        "prompt_file = \"rtp-prompts.txt\"\n",
        "prompt_path = f\"{PATH}/{prompt_file}\"\n",
        "savepath = f\"/content/{MODEL_ID}/{MODEL_ID}-ft-continuations.txt\"\n",
        "  \n",
        "prompts = []\n",
        "for line in open(prompt_path, 'r'):\n",
        "    prompts.append(json.loads(line))\n",
        "N = len(prompts)\n",
        "generator = pipeline('text-generation', model=model_path, device=0)\n",
        "print(\"Generating continuations for {}\".format(MODEL_ID))\n",
        "with open(savepath, 'w') as fp:\n",
        "    for i in tqdm(range(N)):\n",
        "        prompt = prompts[i]['prompt']['text']\n",
        "        continuation = generator(prompt, max_new_tokens = 20, num_return_sequences=1, return_full_text=False)[0]['generated_text'] \n",
        "        output = {\"prompt\": prompt, \"continuation\":continuation}\n",
        "        json.dump(output, fp)\n",
        "        fp.write('\\n')\n",
        "\n",
        "repo.push_to_hub(commit_message=\"Add continuations\")"
      ],
      "metadata": {
        "id": "eLWl4uD01Uqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bGcuKqwa61qh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}